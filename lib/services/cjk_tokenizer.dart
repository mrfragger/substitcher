import 'package:characters/characters.dart';
import 'package:tiny_segmenter_dart/tiny_segmenter_dart.dart';

enum TextLanguage { japanese, korean, chinese, arabic, english, unknown }

class CJKTokenizer {
  static final _tinySegmenter = TinySegmenter();
  
  static const Set<String> _commonWords = {
    "los", "las", "del", "con", "esa", "que",
    "about", "absolutely", "absolute", "abu", "actually", "after", "again", "against", "ago", "aha", "ahl", "all", "allahu", "also", "always", "amongst", "and", "another", "any", "anyone", "anything", "anyway", "are", "around", "ask", "asked", "away", "back", "based", "beautiful", "became", "because", "become", "becomes", "been", "before", "being", "better", "between", "bin", "both", "brings", "brought", "but", "call", "called", "came", "can", "can't", "cannot", "cause", "certain", "clear", "clearly", "close", "comes", "coming", "complete", "correct", "could", "course", "day", "days", "did", "didn't", "different", "does", "doesn't", "doing", "don't", "done", "down", "during", "each", "else", "end", "even", "every", "everything", "everywhere", "exactly", "except", "far", "fell", "few", "find", "first", "for", "form", "four", "from", "full", "gave", "get", "give", "gives", "goes", "going", "got", "great", "had", "has", "have", "having", "hear", "heard", "he'll", "he's", "her", "here", "high", "him", "himself", "his", "happen", "happened", "how", "however", "i'll", "i'm", "i've", "ibn", "important", "indeed", "into", "it's", "its", "just", "keep", "knew", "know", "known", "knows", "last", "later", "left", "let", "let's", "like", "little", "long", "look", "lot", "made", "make", "makes", "many", "may", "maybe", "mean", "means", "might", "more", "most", "much", "need", "never", "new", "next", "non", "nor", "not", "now", "okay", "one", "one's", "only", "order", "other", "others", "our", "out", "over", "own", "part", "past", "proper", "put", "rather", "really", "right", "said", "same", "say", "saying", "says", "see", "sees", "self", "set", "she", "should", "show", "similar", "some", "someone", "something", "sort", "still", "stop", "such", "sure", "take", "taken", "takes", "talk", "talked", "talking", "tell", "than", "that", "that's", "the", "their", "them", "then", "there", "there's", "these", "they", "they'll", "they're", "thing", "things", "think", "third", "this", "those", "though", "today", "too", "took", "true", "try", "two", "type", "under", "understand", "until", "upon", "use", "used", "very", "want", "wants", "was", "wasn't", "way", "we'll", "we're", "we've", "well", "went", "were", "what", "what's", "when", "where", "which", "while", "who", "who's", "whoever", "whom", "why", "will", "with", "within", "without", "word", "would", "year", "years", "yes", "yet", "you", "you'll", "you're", "your", "yourself",
  };

  static bool isCommonWord(String word) {
    final lower = word.toLowerCase().replaceAll("'", "'").replaceAll('`', "'");
    return _commonWords.contains(lower);
  }

  static bool shouldExcludeFromColoring(String word, {TextLanguage? language}) {
    final detectedLang = language ?? detectLanguage(word);
    
    if (detectedLang == TextLanguage.japanese || 
        detectedLang == TextLanguage.chinese || 
        detectedLang == TextLanguage.korean) {
      return false;
    }
    
    return isCommonWord(word) || word.length < 3;
  }
  
  static Future<void> initialize() async {
    print('TinySegmenter ready');
  }
  
  static TextLanguage detectLanguage(String text) {
    bool hasJapanese = false;
    bool hasKorean = false;
    bool hasChinese = false;
    bool hasArabic = false;
    bool hasLatin = false;
    
    for (final char in text.characters) {
      final code = char.runes.first;
      
      if ((code >= 0x3040 && code <= 0x309F) || (code >= 0x30A0 && code <= 0x30FF)) {
        hasJapanese = true;
      } else if ((code >= 0xAC00 && code <= 0xD7AF) || 
                 (code >= 0x1100 && code <= 0x11FF) || 
                 (code >= 0x3130 && code <= 0x318F)) {
        hasKorean = true;
      } else if (code >= 0x4E00 && code <= 0x9FFF) {
        hasChinese = true;
      } else if ((code >= 0x0600 && code <= 0x06FF) || 
                 (code >= 0x0750 && code <= 0x077F) ||
                 (code >= 0xFB50 && code <= 0xFDFF) ||
                 (code >= 0xFE70 && code <= 0xFEFF)) {
        hasArabic = true;
      } else if ((code >= 0x0041 && code <= 0x005A) ||  // A-Z
                 (code >= 0x0061 && code <= 0x007A)) {  // a-z
        hasLatin = true;
      }
    }
    
    TextLanguage detected;
    if (hasJapanese) {
      detected = TextLanguage.japanese;
    } else if (hasKorean) {
      detected = TextLanguage.korean;
    } else if (hasChinese) {
      detected = TextLanguage.chinese;
    } else if (hasArabic && hasLatin) {
      detected = TextLanguage.arabic;
    } else if (hasLatin) {
      detected = TextLanguage.english;
    } else if (hasArabic) {
      detected = TextLanguage.arabic;
    } else {
      detected = TextLanguage.unknown;
    }
    
    // print('ğŸ” Language Detection: "${text.substring(0, text.length > 50 ? 50 : text.length)}" -> $detected (hasArabic: $hasArabic, hasLatin: $hasLatin)');
    
    return detected;
  }

  static List<String> tokenize(String text, {TextLanguage? language}) {
    final detectedLang = language ?? detectLanguage(text);
    
    switch (detectedLang) {
      case TextLanguage.japanese:
        return _tokenizeJapanese(text);
      case TextLanguage.korean:
        return _tokenizeKorean(text);
      case TextLanguage.chinese:
        return _tokenizeChinese(text);
      case TextLanguage.arabic:
        return _tokenizeArabic(text);
      case TextLanguage.english:
      case TextLanguage.unknown:
        return _tokenizeEnglish(text);
    }
  }

  static bool _isSmallKana(String text) {
    if (text.isEmpty) return false;
    for (final char in text.characters) {
      final code = char.runes.first;
      if (code == 0x3041 || code == 0x3043 || code == 0x3045 || code == 0x3047 || code == 0x3049 ||
          code == 0x3083 || code == 0x3085 || code == 0x3087 || code == 0x308E || code == 0x3063 ||
          code == 0x30A1 || code == 0x30A3 || code == 0x30A5 || code == 0x30A7 || code == 0x30A9 ||
          code == 0x30E3 || code == 0x30E5 || code == 0x30E7 || code == 0x30EE || code == 0x30C3) {
        return true;
      }
    }
    return false;
  }

  static bool _startsWithSmallKana(String text) {
    if (text.isEmpty) return false;
    final firstChar = text.characters.first;
    final code = firstChar.runes.first;
    return (code == 0x3041 || code == 0x3043 || code == 0x3045 || code == 0x3047 || code == 0x3049 ||
            code == 0x3083 || code == 0x3085 || code == 0x3087 || code == 0x308E || code == 0x3063 ||
            code == 0x30A1 || code == 0x30A3 || code == 0x30A5 || code == 0x30A7 || code == 0x30A9 ||
            code == 0x30E3 || code == 0x30E5 || code == 0x30E7 || code == 0x30EE || code == 0x30C3);
  }

  static List<String> _tokenizeJapanese(String text) {
    try {
      final segments = _tinySegmenter.segment(text);
      
      final merged = <String>[];
      for (int i = 0; i < segments.length; i++) {
        final current = segments[i].trim();
        if (current.isEmpty) continue;
        
        if (!RegExp(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF\w]').hasMatch(current)) {
          continue;
        }
        
        if (merged.isEmpty) {
          merged.add(current);
          continue;
        }
        
        if (_startsWithSmallKana(current) || current == 'ã‚“' || current == 'ãƒ³') {
          merged[merged.length - 1] = merged.last + current;
        } else {
          merged.add(current);
        }
      }
      
      return merged;
    } catch (e) {
      print('TinySegmenter error: $e');
      return [text];
    }
  }

  static List<String> _tokenizeChinese(String text) {
    final words = <String>[];
    final chars = text.characters.toList();
    
    int i = 0;
    while (i < chars.length) {
      if (!_isChineseChar(chars[i])) {
        if (RegExp(r'[a-zA-Z0-9]').hasMatch(chars[i])) {
          final buffer = StringBuffer(chars[i]);
          i++;
          while (i < chars.length && RegExp(r'[a-zA-Z0-9]').hasMatch(chars[i])) {
            buffer.write(chars[i]);
            i++;
          }
          words.add(buffer.toString());
        } else {
          i++;
        }
        continue;
      }
      
      int maxMatchLen = 0;
      String? bestMatch;
      
      for (int len = 4; len >= 1; len--) {
        if (i + len > chars.length) continue;
        
        final candidate = chars.sublist(i, i + len).join();
        if (_isCommonChineseWord(candidate, len)) {
          maxMatchLen = len;
          bestMatch = candidate;
          break;
        }
      }
      
      if (bestMatch != null && maxMatchLen > 0) {
        words.add(bestMatch);
        i += maxMatchLen;
      } else {
        words.add(chars[i]);
        i++;
      }
    }
    
    return words.where((w) => w.trim().isNotEmpty).toList();
  }

  static bool _isChineseChar(String char) {
    final code = char.runes.first;
    return code >= 0x4E00 && code <= 0x9FFF;
  }

  static bool _isCommonChineseWord(String word, int len) {
    if (len == 4) return _chinese4CharWords.contains(word);
    if (len == 3) return _chinese3CharWords.contains(word);
    if (len == 2) return _chinese2CharWords.contains(word);
    return false;
  }

  static final Set<String> _chinese2CharWords = {
    'ä½ ä»¬', 'ä»–ä»¬', 'å¥¹ä»¬', 'æˆ‘ä»¬', 'å’±ä»¬', 'äººä»¬', 'å¤§å®¶', 'è‡ªå·±', 'åˆ«äºº',
    'ä»€ä¹ˆ', 'æ€ä¹ˆ', 'ä¸ºä»€', 'è¿™ä¸ª', 'é‚£ä¸ª', 'å“ªä¸ª', 'è¿™äº›', 'é‚£äº›', 'å“ªäº›',
    'å¯ä»¥', 'åº”è¯¥', 'å¿…é¡»', 'éœ€è¦', 'å¸Œæœ›', 'æƒ³è¦', 'å–œæ¬¢', 'è§‰å¾—', 'è®¤ä¸º',
    'çŸ¥é“', 'æ˜ç™½', 'äº†è§£', 'ç†è§£', 'ç›¸ä¿¡', 'ä»¥ä¸º', 'å¬è¯´', 'çœ‹è§', 'é‡åˆ°',
    'æ—¶å€™', 'åœ°æ–¹', 'æ–¹é¢', 'æƒ…å†µ', 'é—®é¢˜', 'äº‹æƒ…', 'ä¸œè¥¿', 'æ–¹æ³•', 'åŠæ³•',
    'å·²ç»', 'è¿˜æ˜¯', 'æˆ–è€…', 'è€Œä¸”', 'ä½†æ˜¯', 'å› ä¸º', 'æ‰€ä»¥', 'å¦‚æœ', 'è™½ç„¶',
    'éå¸¸', 'å¾ˆå¤š', 'ä¸€äº›', 'å‡ ä¸ª', 'ä¸€ç‚¹', 'æœ‰ç‚¹', 'å¤ªå¤š', 'æ›´å¤š', 'æœ€å¤š',
    'ä»Šå¤©', 'æ˜å¤©', 'æ˜¨å¤©', 'ç°åœ¨', 'ä»¥å‰', 'ä»¥å', 'åˆšæ‰', 'é©¬ä¸Š', 'ç«‹åˆ»',
    'ä¸­å›½', 'ç¾å›½', 'æ—¥æœ¬', 'è‹±å›½', 'æ³•å›½', 'å¾·å›½', 'ä¿„å›½', 'éŸ©å›½', 'å°åº¦',
    'å¼€å§‹', 'ç»“æŸ', 'ç»§ç»­', 'åœæ­¢', 'å‘ç”Ÿ', 'å‡ºç°', 'å­˜åœ¨', 'äº§ç”Ÿ', 'è¿›è¡Œ',
    'å·¥ä½œ', 'å­¦ä¹ ', 'ç”Ÿæ´»', 'æ—…è¡Œ', 'è¿åŠ¨', 'ä¼‘æ¯', 'ç¡è§‰', 'åƒé¥­', 'å–æ°´',
    'å­¦æ ¡', 'å…¬å¸', 'åŒ»é™¢', 'é“¶è¡Œ', 'å•†åº—', 'é¥­åº—', 'é…’åº—', 'æœºåœº', 'è½¦ç«™',
    'æœ‹å‹', 'å®¶äºº', 'çˆ¶æ¯', 'å­©å­', 'è€å¸ˆ', 'å­¦ç”Ÿ', 'åŒäº‹', 'é‚»å±…', 'å®¢äºº',
    'æ‰‹æœº', 'ç”µè„‘', 'ç½‘ç»œ', 'è§†é¢‘', 'éŸ³ä¹', 'ç”µå½±', 'ä¹¦ç±', 'æŠ¥çº¸', 'æ‚å¿—',
    'çª—æˆ·', 'å¨æˆ¿', 'é—¨ä¹Ÿ', 'æ²¡å…³', 'çœ‹çª—', 'å…³ç³»',
  };

  static final Set<String> _chinese3CharWords = {
    'æ€ä¹ˆæ ·', 'ä¸ºä»€ä¹ˆ', 'æ²¡å…³ç³»', 'ä¸å®¢æ°”', 'å¯¹ä¸èµ·', 'æ²¡ä»€ä¹ˆ', 'ä¸çŸ¥é“',
    'å¾ˆé«˜å…´', 'å¤ªå¥½äº†', 'çœŸä¸é”™', 'æ²¡é—®é¢˜', 'å½“ç„¶äº†', 'å¯èƒ½æ˜¯', 'åº”è¯¥æ˜¯',
    'å¥½æœ‹å‹', 'è€æœ‹å‹', 'æ–°æœ‹å‹', 'å¥½å­©å­', 'å°å­©å­', 'å¹´è½»äºº', 'è€å¹´äºº',
    'ä¸­å­¦ç”Ÿ', 'å¤§å­¦ç”Ÿ', 'ç ”ç©¶ç”Ÿ', 'ç•™å­¦ç”Ÿ', 'å¤–å›½äºº', 'ä¸­å›½äºº', 'ç¾å›½äºº',
    'äº’è”ç½‘', 'å›¾ä¹¦é¦†', 'ç«è½¦ç«™', 'æ±½è½¦ç«™', 'é£æœºåœº', 'å‡ºç§Ÿè½¦',
  };

  static final Set<String> _chinese4CharWords = {
    'æ™ºèƒ½æ‰‹æœº', 'ç”µå­é‚®ä»¶', 'ç¤¾äº¤åª’ä½“', 'äººå·¥æ™ºèƒ½',
  };

  static List<String> _tokenizeKorean(String text) {
    final words = <String>[];
    final buffer = StringBuffer();
    bool inHangul = false;
    
    for (final char in text.characters) {
      final charCode = char.runes.first;
      
      if ((charCode >= 0xAC00 && charCode <= 0xD7AF) ||
          (charCode >= 0x1100 && charCode <= 0x11FF) ||
          (charCode >= 0x3130 && charCode <= 0x318F)) {
        buffer.write(char);
        inHangul = true;
      }
      else if (RegExp(r'[a-zA-Z0-9]').hasMatch(char)) {
        if (inHangul && buffer.isNotEmpty) {
          words.add(buffer.toString());
          buffer.clear();
        }
        buffer.write(char);
        inHangul = false;
      }
      else {
        if (buffer.isNotEmpty) {
          words.add(buffer.toString());
          buffer.clear();
        }
        inHangul = false;
      }
    }
    
    if (buffer.isNotEmpty) {
      words.add(buffer.toString());
    }
    
    return words.where((w) => w.trim().isNotEmpty).toList();
  }

  static List<String> _tokenizeArabic(String text) {
    final cleaned = text
        .replaceAll(RegExp(r'[\u200B-\u200F\u202A-\u202E\u2060-\u206F]'), '')
        .trim();
    
    final words = cleaned
        .split(RegExp(r'\s+'))
        .map((word) {

          return word.replaceAll(RegExp(r'^[.,!?;:\-ØŒØŸØ›ÙªÛ”\s]+|[.,!?;:\-ØŒØŸØ›ÙªÛ”\s]+$'), '');
        })
        .where((w) => w.isNotEmpty)
        .toList();
    
    print('ğŸ“ Tokenized Arabic: ${words.length} words: $words');
    return words;
  }

  static List<String> _tokenizeEnglish(String text) {
    final cleaned = text
        .replaceAll(RegExp(r'[\u200B-\u200F\u202A-\u202E\u2060-\u206F]'), '')
        .trim();
    
    final words = cleaned
        .split(RegExp(r'\s+'))
        .map((word) {
          return word.replaceAll(RegExp(r"^[^\w`'\-]+|[^\w`'\-]+$"), '');
        })
        .where((w) => w.isNotEmpty)
        .toList();
    
    print('ğŸ“ Tokenized English: ${words.length} words: $words');
    return words;
  }

  static List<String> getLongerVariations(List<String> words, String text, int startIndex) {
    final variations = <String>[words[startIndex]];
    
    if (startIndex + 1 < words.length) {
      variations.add('${words[startIndex]} ${words[startIndex + 1]}');
    }
    
    if (startIndex + 2 < words.length) {
      variations.add('${words[startIndex]} ${words[startIndex + 1]} ${words[startIndex + 2]}');
    }
    
    return variations;
  }
}